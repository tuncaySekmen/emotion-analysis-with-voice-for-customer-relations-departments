{"cells":[{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3318,"status":"ok","timestamp":1735506010140,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"uE_oqYeMjm4k","outputId":"d39bf200-3e81-4c90-ac78-3dea110d5aa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4169,"status":"ok","timestamp":1735506014305,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"jbQGA05UvYZH","outputId":"45aab177-4fcc-4a80-eb79-411ba20e1045"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install torch torchaudio transformers datasets\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2824,"status":"ok","timestamp":1735506017126,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"VX7sObPtvzna","outputId":"c7de711e-ac3f-44a4-fe64-873f7be9cc9b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["HubertForSequenceClassification(\n","  (hubert): HubertModel(\n","    (feature_extractor): HubertFeatureEncoder(\n","      (conv_layers): ModuleList(\n","        (0): HubertLayerNormConvLayer(\n","          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n","          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation): GELUActivation()\n","        )\n","        (1-4): 4 x HubertLayerNormConvLayer(\n","          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n","          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation): GELUActivation()\n","        )\n","        (5-6): 2 x HubertLayerNormConvLayer(\n","          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n","          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (activation): GELUActivation()\n","        )\n","      )\n","    )\n","    (feature_projection): HubertFeatureProjection(\n","      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (projection): Linear(in_features=512, out_features=1024, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): HubertEncoderStableLayerNorm(\n","      (pos_conv_embed): HubertPositionalConvEmbedding(\n","        (conv): ParametrizedConv1d(\n","          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (padding): HubertSamePadLayer()\n","        (activation): GELUActivation()\n","      )\n","      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (layers): ModuleList(\n","        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n","          (attention): HubertSdpaAttention(\n","            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (feed_forward): HubertFeedForward(\n","            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n","            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (output_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (projector): Linear(in_features=1024, out_features=256, bias=True)\n","  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",")"]},"metadata":{},"execution_count":18}],"source":["from transformers import Wav2Vec2Processor, HubertForSequenceClassification\n","import torch\n","import numpy as np\n","\n","# Model ve işlemci yükleniyor\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n","model = HubertForSequenceClassification.from_pretrained(\"facebook/hubert-large-ls960-ft\", num_labels=3)  # 3 sınıf için\n","model.to(\"cuda\")  # GPU kullanımı\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1476,"status":"ok","timestamp":1735506018599,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"mL4IZCtdziLK","outputId":"46076da1-0701-45e4-8c25-5a194827c9d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Toplam 6010 dosya bulundu.\n"]}],"source":["import glob\n","\n","# Ses dosyalarının yollarını listele\n","agresif_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/agresif/*.wav\")\n","huzunlu_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/huzunlu/*.wav\")\n","neseli_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/neseli/*.wav\")\n","\n","# Tüm dosyaları ve etiketlerini birleştir\n","audio_paths = agresif_files + huzunlu_files + neseli_files\n","labels = [0] * len(agresif_files) + [1] * len(huzunlu_files) + [2] * len(neseli_files)\n","\n","print(f\"Toplam {len(audio_paths)} dosya bulundu.\")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1187,"status":"ok","timestamp":1735506019782,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"EC4E4f4S43m3"},"outputs":[],"source":["import torchaudio\n","from transformers import Wav2Vec2Processor\n","\n","# HuBERT için işlemci\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n","\n","def preprocess_audio(audio_path):\n","    # Ses dosyasını yükle\n","    waveform, sample_rate = torchaudio.load(audio_path)\n","    # 16kHz'e yeniden örnekleme\n","    if sample_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n","        waveform = resampler(waveform)\n","    # Modelin beklediği şekilde işleme\n","    inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n","    return inputs\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1735506020432,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"KsSLSJCC5LI5","outputId":"4a6be894-ab78-401e-e1e1-8f52214a609a"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_values': tensor([[-0.5919, -1.9172, -1.8174,  ..., -0.2172,  0.8590, -0.1859],\n","        [-0.7631, -2.0116, -1.8164,  ...,  2.1737,  1.2146,  0.8881]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1]], dtype=torch.int32)}\n"]}],"source":["# İlk ses dosyasını test etmek için:\n","inputs = preprocess_audio(audio_paths[0])\n","print(inputs)\n"]},{"cell_type":"markdown","metadata":{"id":"CT57xTkY5-Vi"},"source":["**Veri Kümesini İşleme**"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1735506020432,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"vm4aRA6V5hr5"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class AudioDataset(Dataset):\n","    def __init__(self, audio_paths, labels):\n","        self.audio_paths = audio_paths\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.audio_paths)\n","\n","    def __getitem__(self, idx):\n","        # Ses dosyasını işleme\n","        inputs = preprocess_audio(self.audio_paths[idx])\n","        # Etiket\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        # İşlenmiş girişleri ve etiketi döndür\n","        return inputs, label\n"]},{"cell_type":"markdown","metadata":{"id":"Byk7dfKO6HnX"},"source":["**DataLoader ile Verileri Hazırlama**"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14569,"status":"ok","timestamp":1735506034997,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"f-vlsfDI6Dw_","outputId":"2460a461-8331-47d8-83bf-660aa915f75a"},"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['input_values', 'attention_mask'])\n","tensor([0, 0, 0, 2, 1, 2, 2, 0, 0, 2, 1, 0, 1, 1, 0, 2])\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","# Dataset oluştur\n","dataset = AudioDataset(audio_paths, labels)\n","\n","# DataLoader oluştur\n","data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n","\n","# İlk batch'i test et\n","for batch in data_loader:\n","    inputs, labels = batch\n","    print(inputs.keys())  # input_values ve attention_mask\n","    print(labels)         # Sınıf etiketleri\n","    break\n"]},{"cell_type":"markdown","metadata":{"id":"Mjw5duAe6QzR"},"source":["**Model Eğitimine Geçiş**"]},{"cell_type":"markdown","metadata":{"id":"Y9lCfkhZLskL"},"source":["16000*15sn=240000 : sabit uzunluğa getirme\n","\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1735506034997,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"nuofp4Bc6Qpg"},"outputs":[],"source":["import torch\n","\n","def pad_or_truncate(waveform, target_length=240000):\n","    \"\"\"\n","    Ses dalgasını sabit bir uzunluğa getirir.\n","    - waveform: Ses dalgası tensörü\n","    - target_length: Hedef uzunluk (ör. 15 saniye için 16kHz * 15 = 240000)\n","    \"\"\"\n","    current_length = waveform.size(-1)\n","    if current_length > target_length:\n","        # Kırpma\n","        waveform = waveform[:, :target_length]\n","    elif current_length < target_length:\n","        # Doldurma\n","        padding = target_length - current_length\n","        waveform = torch.nn.functional.pad(waveform, (0, padding))\n","    return waveform\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1735506034997,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"2TTEI1af6VBe"},"outputs":[],"source":["def preprocess_audio(audio_path, target_length=240000):\n","    # Ses dosyasını yükle\n","    waveform, sample_rate = torchaudio.load(audio_path)\n","\n","    # 16kHz'e yeniden örnekleme\n","    if sample_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n","        waveform = resampler(waveform)\n","\n","    # Sabit uzunluğa getir\n","    waveform = pad_or_truncate(waveform, target_length=target_length)\n","\n","    # Modelin beklediği şekilde işleme\n","    inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n","    return inputs\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13674,"status":"ok","timestamp":1735506048667,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"H_lRVUUgMEOa","outputId":"09fa20b3-8ac1-4004-de46-36cc27dbdae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 2, 240000])\n","tensor([0, 0, 1, 2, 2, 0, 2, 0, 1, 1, 2, 0, 1, 2, 0, 1])\n"]}],"source":["# İlk batch'i test et\n","for batch in data_loader:\n","    inputs, labels = batch\n","    print(inputs[\"input_values\"].shape)  # Sabit uzunlukta tensör\n","    print(labels)\n","    break\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1735506048667,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"FPzhjil5MGHs"},"outputs":[],"source":["# Mono'ya dönüştür: Kanalları ortalama alarak birleştirme\n","input_values = inputs[\"input_values\"].mean(dim=1).to(\"cuda\")  # Şekil: [batch_size, sequence_length]\n","attention_mask = inputs[\"attention_mask\"][:, 0, :].to(\"cuda\")  # İlk kanalın maskesi\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NwYbjCE4Qzw7"},"source":["**Model Eğitim Döngüsü**"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1735506048667,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"},"user_tz":-180},"id":"HhxIWIg8UOD6","outputId":"d8937a37-0c67-41fa-c750-44236b9f43e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["input_values.shape: torch.Size([16, 240000])\n","attention_mask.shape: torch.Size([16, 240000])\n"]}],"source":["print(f\"input_values.shape: {input_values.shape}\")  # Beklenen: [batch_size, sequence_length]\n","print(f\"attention_mask.shape: {attention_mask.shape}\")  # Beklenen: [batch_size, sequence_length]\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1slRm_THEHfv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456},"id":"UxM8JoatM0TP","executionInfo":{"status":"error","timestamp":1735510864497,"user_tz":-180,"elapsed":3263729,"user":{"displayName":"Tuncay Sekmen","userId":"07388451099079562278"}},"outputId":"63cb01e7-51d3-4a05-e5fe-1a8ba4b910c5"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Training Loss: 0.8731312045638454\n","Validation Loss: 0.7502613568622805, Accuracy: 67.47%\n","Epoch 2, Training Loss: 0.6607314028874808\n","Validation Loss: 0.5401880948052454, Accuracy: 80.12%\n","Epoch 3, Training Loss: 0.5401198153884558\n","Validation Loss: 0.5519181924850441, Accuracy: 78.04%\n","Confusion Matrix:\n","[[352  15  63]\n"," [ 27 295  65]\n"," [ 79  15 291]]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'label_binarize' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-795f71129207>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m ])\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ovr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall (Sensitivity): {recall:.2f}, F1-Score: {f1:.2f}, Specificity: {specificity:.2f}, AUC: {auc:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'label_binarize' is not defined"]}],"source":["from transformers import HubertForSequenceClassification, Wav2Vec2Processor\n","from torch.utils.data import DataLoader, Dataset\n","from torch.amp import GradScaler, autocast\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_score,\n","    recall_score, f1_score, accuracy_score\n",")\n","import torch\n","import glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","\n","# Eğitim ve çıkarım zamanı hesaplama için timer\n","start_time = time.time()\n","\n","# 1. Verilerin Hazırlanması\n","class AudioDataset(Dataset):\n","    def __init__(self, audio_paths, labels):\n","        self.audio_paths = audio_paths\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.audio_paths)\n","\n","    def __getitem__(self, idx):\n","        inputs = preprocess_audio(self.audio_paths[idx])\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return inputs, label\n","\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n","def preprocess_audio(audio_path, target_length=240000):  # 15 saniye = 16kHz * 15\n","    waveform, sample_rate = torchaudio.load(audio_path)\n","    if sample_rate != 16000:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n","        waveform = resampler(waveform)\n","    if waveform.size(1) > target_length:\n","        waveform = waveform[:, :target_length]\n","    elif waveform.size(1) < target_length:\n","        padding = target_length - waveform.size(1)\n","        waveform = torch.nn.functional.pad(waveform, (0, padding))\n","    inputs = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n","    return inputs\n","\n","# 2. Verilerin Tanımlanması\n","agresif_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/agresif/*.wav\")\n","huzunlu_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/huzunlu/*.wav\")\n","neseli_files = glob.glob(\"/content/drive/MyDrive/Yazgel_Muzik_Secilen_1000/neseli/*.wav\")\n","\n","audio_paths = agresif_files + huzunlu_files + neseli_files\n","labels = [0] * len(agresif_files) + [1] * len(huzunlu_files) + [2] * len(neseli_files)\n","\n","from sklearn.model_selection import train_test_split\n","train_paths, val_paths, train_labels, val_labels = train_test_split(audio_paths, labels, test_size=0.2, random_state=42)\n","\n","train_dataset = AudioDataset(train_paths, train_labels)\n","val_dataset = AudioDataset(val_paths, val_labels)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n","\n","# 3. Model, Optimizasyon ve Kayıp Fonksiyonu\n","model = HubertForSequenceClassification.from_pretrained(\"facebook/hubert-large-ls960-ft\", num_labels=3)\n","model.to(\"cuda\")\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","scaler = GradScaler()\n","\n","# 4. Eğitim ve Validasyon\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(3):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_loader:\n","        inputs, labels = batch\n","        input_values = inputs[\"input_values\"].mean(dim=1).to(\"cuda\")\n","        attention_mask = inputs[\"attention_mask\"][:, 0, :].to(\"cuda\")\n","        labels = labels.to(\"cuda\")\n","\n","        with autocast(device_type=\"cuda\"):\n","            outputs = model(input_values=input_values, attention_mask=attention_mask)\n","            loss = loss_fn(outputs.logits, labels)\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","    train_losses.append(total_loss / len(train_loader))\n","    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\")\n","\n","    # Validasyon\n","    model.eval()\n","    val_loss = 0\n","    correct = 0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            inputs, labels = batch\n","            input_values = inputs[\"input_values\"].mean(dim=1).to(\"cuda\")\n","            attention_mask = inputs[\"attention_mask\"][:, 0, :].to(\"cuda\")\n","            labels = labels.to(\"cuda\")\n","\n","            outputs = model(input_values=input_values, attention_mask=attention_mask)\n","            val_loss += loss_fn(outputs.logits, labels).item()\n","            preds = torch.argmax(outputs.logits, dim=1)\n","\n","            correct += (preds == labels).sum().item()\n","            true_labels.extend(labels.cpu().numpy())\n","            predictions.extend(preds.cpu().numpy())\n","\n","    val_losses.append(val_loss / len(val_loader))\n","    print(f\"Validation Loss: {val_loss / len(val_loader)}, Accuracy: {100 * correct / len(val_loader.dataset):.2f}%\")\n","\n","# 5. Performans Metrikleri ve Çıktılar\n","conf_matrix = confusion_matrix(true_labels, predictions)\n","print(f\"Confusion Matrix:\\n{conf_matrix}\")\n","\n","accuracy = accuracy_score(true_labels, predictions)\n","precision = precision_score(true_labels, predictions, average='macro')\n","recall = recall_score(true_labels, predictions, average='macro')\n","f1 = f1_score(true_labels, predictions, average='macro')\n","specificity = np.mean([\n","    conf_matrix[i, i] / (np.sum(conf_matrix[:, i]) - conf_matrix[i, i] + conf_matrix[i, i])\n","    for i in range(3)\n","])\n","auc = roc_auc_score(label_binarize(true_labels, classes=[0, 1, 2]), label_binarize(predictions, classes=[0, 1, 2]), multi_class=\"ovr\")\n","\n","print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall (Sensitivity): {recall:.2f}, F1-Score: {f1:.2f}, Specificity: {specificity:.2f}, AUC: {auc:.2f}\")\n","\n","# ROC Eğrisi ve Loss Grafiği\n","for i in range(3):\n","    fpr, tpr, _ = roc_curve(label_binarize(true_labels, classes=[0, 1, 2])[:, i], label_binarize(predictions, classes=[0, 1, 2])[:, i])\n","    plt.plot(fpr, tpr, label=f\"Class {i}\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve\")\n","plt.legend()\n","plt.show()\n","\n","plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n","plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss vs Epoch\")\n","plt.legend()\n","plt.show()\n","\n","print(f\"Training Time: {time.time() - start_time:.2f} seconds\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPKfgnbR7CgrRt/vE/cLR28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}